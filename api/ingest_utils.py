# api/ingest_utils.py
import os
from typing import List, Dict, Any

from api.evtx_parser import generate_evtx_derivatives
from api.registry_parser import generate_registry_derivatives
from api.embedder import embed_texts

TEXT_EXTENSIONS = {".txt", ".log", ".json", ".csv", ".md"}
REGISTRY_EXTENSIONS = {".dat", ".hiv", ".hive", ".reg"}

MAX_FILE_CHARS = int(os.getenv("MAX_FILE_CHARS", "20000"))       # safety for huge files
CHUNK_CHARS = int(os.getenv("CHUNK_CHARS", "1200"))
CHUNK_OVERLAP = int(os.getenv("CHUNK_OVERLAP", "150"))


def _chunk_text(text: str, max_chars: int = CHUNK_CHARS, overlap: int = CHUNK_OVERLAP) -> List[str]:
    text = (text or "").strip()
    if not text:
        return []
    if len(text) <= max_chars:
        return [text]

    out = []
    step = max(1, max_chars - overlap)
    for start in range(0, len(text), step):
        chunk = text[start : start + max_chars].strip()
        if chunk:
            out.append(chunk)
    return out


def _read_text_file(path: str) -> str:
    try:
        with open(path, "r", encoding="utf-8", errors="ignore") as f:
            return f.read(MAX_FILE_CHARS)
    except Exception:
        return ""


def build_and_index_case_corpus(case_dir: str, case_id: str) -> int:
    """
    Walk the case directory (prefer case_dir/files), convert EVTX + Registry → text,
    write summaries (evtx_summaries.jsonl / registry_summaries.jsonl),
    and push chunks into Chroma via embed_texts().

    Returns number of text chunks indexed.
    """
    text_chunks: List[str] = []
    metadata_list: List[Dict[str, Any]] = []

    # Prefer scanning extracted evidence in /files only (prevents feedback loops)
    scan_root = os.path.join(case_dir, "files")
    if not os.path.isdir(scan_root):
        scan_root = case_dir

    evtx_summary_path = os.path.join(case_dir, "evtx_summaries.jsonl")
    reg_summary_path = os.path.join(case_dir, "registry_summaries.jsonl")

    with open(evtx_summary_path, "w", encoding="utf-8") as evtx_summary_f, \
         open(reg_summary_path, "w", encoding="utf-8") as reg_summary_f:

        for root, _, files in os.walk(scan_root):
            for filename in files:
                path = os.path.join(root, filename)
                ext = os.path.splitext(filename)[1].lower()
                rel_path = os.path.relpath(path, case_dir)

                # Skip our own outputs if scanning case_dir directly
                if filename in ("evtx_summaries.jsonl", "registry_summaries.jsonl", "metadata.jsonl"):
                    continue

                # 1) EVTX
                if ext == ".evtx":
                    stats = generate_evtx_derivatives(path, case_dir)
                    print(f"[EVTX] {filename}: {stats['events_count']} events parsed")

                    # Index the one-line summaries generated by evtx_parser
                    try:
                        with open(stats["txt_path"], "r", encoding="utf-8", errors="ignore") as f:
                            for line in f:
                                line = line.strip()
                                if not line:
                                    continue
                                text_chunks.append(line)
                                metadata_list.append({
                                    "source": "evtx",
                                    "case_id": case_id,
                                    "file": rel_path,
                                })
                                evtx_summary_f.write(line + "\n")
                    except Exception as e:
                        print(f"[EVTX] failed reading derivative txt for {filename}: {e}")

                # 2) Registry
                elif ext in REGISTRY_EXTENSIONS:
                    print(f"[REGISTRY] candidate: {filename}")
                    stats = generate_registry_derivatives(path, case_dir)
                    print(f"[REGISTRY] {filename}: {stats['events_count']} entries parsed")

                    if stats.get("events_count", 0) > 0:
                        try:
                            with open(stats["txt_path"], "r", encoding="utf-8", errors="ignore") as f:
                                for line in f:
                                    line = line.strip()
                                    if not line:
                                        continue
                                    text_chunks.append(line)
                                    metadata_list.append({
                                        "source": "registry",
                                        "case_id": case_id,
                                        "file": rel_path,
                                    })
                                    reg_summary_f.write(line + "\n")
                        except Exception as e:
                            print(f"[REGISTRY] failed reading derivative txt for {filename}: {e}")

                # 3) Normal text-like files
                elif ext in TEXT_EXTENSIONS:
                    content = _read_text_file(path)
                    if not content.strip():
                        continue

                    # Chunk so embeddings don’t become garbage
                    chunks = _chunk_text(content)
                    for idx, ch in enumerate(chunks):
                        text_chunks.append(ch)
                        metadata_list.append({
                            "source": "file",
                            "case_id": case_id,
                            "file": rel_path,
                            "chunk": idx,
                        })

    # Embed in safe batches
    if text_chunks:
        max_batch = 2000
        total = len(text_chunks)
        for start in range(0, total, max_batch):
            end = min(start + max_batch, total)
            print(f"[EMBED] case={case_id} batch={start}-{end-1} of {total}")
            embed_texts(case_id, text_chunks[start:end], metadata_list[start:end])

    return len(text_chunks)
